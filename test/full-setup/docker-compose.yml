#                                                                         __
# .-----.-----.______.-----.----.-----.--.--.--.--.______.----.---.-.----|  |--.-----.
# |  _  |  _  |______|  _  |   _|  _  |_   _|  |  |______|  __|  _  |  __|     |  -__|
# |___  |_____|      |   __|__| |_____|__.__|___  |      |____|___._|____|__|__|_____|
# |_____|            |__|                   |_____|
#
# Copyright (c) 2023 Fabio Cicerchia. https://fabiocicerchia.it. MIT License
# Repo: https://github.com/fabiocicerchia/go-proxy-cache

services:
  # CORE - GOPROXYCACHE
  ##############################################################################
  go-proxy-cache:
    build:
      context: ../..
      dockerfile: ./docker/Dockerfile
      args:
        BUILD_CMD: build-race
    command: ["go-proxy-cache", "-debug"]
    environment:
      TRACING_ENV: test
    depends_on:
      redis:
        condition: service_healthy
      jaeger:
        condition: service_healthy
    ports:
      - "50080:50080" # HTTP
      - "50443:50443" # HTTPS
      - "52021:52021" # GPC Internals
    volumes:
      - ./config.yml:/app/config.yml
      - ./certs:/app/certs

  # STORAGE - REDIS
  ##############################################################################
  redis:
    image: redis:7.0.9-alpine3.17
    ports:
      - "6379:6379" # Redis
    sysctls:
      # WARNING: The TCP backlog setting of 511 cannot be enforced because
      # /proc/sys/net/core/somaxconn is set to the lower value of 128.
      net.core.somaxconn: 1024
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30

  # STORAGE - REDIS CLUSTER
  ##############################################################################
  redis-cluster:
      image: 'redis:6.2.12-alpine3.17'
      command: redis-cli --cluster create 172.20.0.36:6379 172.20.0.37:6379 172.20.0.38:6379 --cluster-replicas 0 --cluster-yes
      networks:
        app_subnet:
          ipv4_address: 172.20.0.30
      depends_on:
        - redisCluster1
        - redisCluster2
        - redisCluster3

  redisCluster1:
    image: redis:6.2.12-alpine3.17
    command: ["redis-server", "--appendonly", "yes", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000"]
    ports:
      - "6380:6379"
    networks:
      app_subnet:
        ipv4_address: 172.20.0.36
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30

  redisCluster2:
    image: redis:6.2.12-alpine3.17
    command: ["redis-server", "--appendonly", "yes", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000"]
    ports:
      - "6381:6379"
    networks:
      app_subnet:
        ipv4_address: 172.20.0.37
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30

  redisCluster3:
    image: redis:6.2.12-alpine3.17
    command: ["redis-server", "--appendonly", "yes", "--cluster-enabled", "yes", "--cluster-config-file", "nodes.conf", "--cluster-node-timeout", "5000"]
    ports:
      - "6382:6379"
    networks:
      app_subnet:
        ipv4_address: 172.20.0.38
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30

  # UPSTREAM 1 - NGINX
  ##############################################################################
  nginx:
    build:
      context: .
      dockerfile: ./docker/Dockerfile.nginx
    restart: always
    depends_on:
      jaeger:
        condition: service_healthy
    volumes:
      - ./nginx/otel-nginx.toml:/conf/otel-nginx.toml
      - ./nginx/conf.d/vhost.conf:/etc/nginx/conf.d/vhost.conf
      - ./nginx/.htpasswd:/etc/nginx/.htpasswd
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./certs:/certs
    ports:
      - "40080:40080" # HTTP
      - "40443:40443" # HTTPS
      - "40081:40081" # WS
      - "40082:40082" # WSS

  # UPSTREAM 2 - NODEJS
  ##############################################################################
  node:
    image: node:19.7.0-alpine3.17
    restart: always
    command: "npm start"
    user: "1000:1000"
    working_dir: /home/app
    depends_on:
      jaeger:
        condition: service_healthy
    volumes:
      - ./ws:/home/app
      - ./certs:/home/app/certs
    ports:
      - "9001:9001" # WS
      - "9002:9002" # WSS

  # TRACING - COLLECTOR
  ##############################################################################
  collector:
    # TODO: the 0.69.0 is not working with nginx configuration (need to find the correct otel_ngx_module.so)
    # image: otel/opentelemetry-collector:0.69.0
    image: otel/opentelemetry-collector:0.35.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./collector/collector.yml:/etc/otel-collector-config.yaml
    ports:
      - "1888:1888"   # pprof extension
      - "8888:8888"   # Prometheus metrics exposed by the collector
      - "8889:8889"   # Prometheus exporter metrics
      - "13133:13133" # health_check extension
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP http receiver
      - "55679:55679" # zpages extension

  # TRACING - JAEGER
  ##############################################################################
  jaeger:
    image: jaegertracing/all-in-one:1.42.0
    ports:
      - "16686:16686"
    healthcheck:
      test: ["CMD", "/go/bin/all-in-one-linux", "status"]
      timeout: 5s
      retries: 5
      start_period: 10s

  # METRICS - PROMETHEUS
  ##############################################################################
  prometheus:
    image: prom/prometheus:v2.42.0
    ports:
      - "9090:9090" # Prometheus
    command: --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml

  # DASHBOARD - GRAFANA
  ##############################################################################
  grafana:
    image: grafana/grafana:9.3.8-ubuntu
    ports:
      - "3001:3000" # Grafana
    volumes:
      - ../../grafana/provisioning:/etc/grafana/provisioning

  # LOGS - SYSLOG
  ##############################################################################
  syslog:
    image: pbertera/syslogserver:latest
    environment:
      SYSLOG_USERNAME: admin
      SYSLOG_PASSWORD: 1234
    ports:
      - "8080:80" # pimpmylogs
      - "514:514/udp" # rsyslog

  # LOGS - SENTRY
  ##############################################################################
  sentry:
    image: sentry:9.1.2
    depends_on:
      - postgres
      - redis
    environment:
      SENTRY_SECRET_KEY: test
      SENTRY_REDIS_HOST: redis
      SENTRY_POSTGRES_HOST: postgres
      SENTRY_DB_USER: sentry
      SENTRY_DB_PASSWORD: secret
    ports:
      - "9000:9000" # sentry
  postgres:
    image: postgres:15.2-alpine3.17
    environment:
      POSTGRES_USER: sentry
      POSTGRES_PASSWORD: secret
    ports:
      - "5432:5432" # postgres

networks:
  app_subnet:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
